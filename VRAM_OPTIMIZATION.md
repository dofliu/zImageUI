# VRAM å„ªåŒ–å®Œå…¨æŒ‡å— - æ¶ˆé™¤å…±ç”¨è¨˜æ†¶é«”æº¢å‡º

## ğŸ” å•é¡Œåˆ†æ

æ‚¨çš„ GPU ç‹€æ…‹:
```
å°ˆå±¬ GPU è¨˜æ†¶é«”: 11.7/12.0 GB (98% ä½¿ç”¨)
å…±ç”¨ GPU è¨˜æ†¶é«”: 1.8/15.8 GB (æº¢å‡ºåˆ°ç³»çµ± RAM)
ç¸½ä½¿ç”¨: 13.5 GB
```

**å•é¡Œ**: å³ä½¿ä½¿ç”¨ CPU Offload,ä»æœ‰ç´„ 1.8GB æº¢å‡ºåˆ°ç³»çµ±è¨˜æ†¶é«”,å°è‡´é€Ÿåº¦è®Šæ…¢ã€‚

## âœ… å·²å¯¦æ–½çš„å„ªåŒ– (Level 1)

### 1. Sequential CPU Offload (æœ€æ¿€é€²)
```python
pipe.enable_sequential_cpu_offload()
```

**æ¯”è¼ƒ**:
- `enable_model_cpu_offload()`: æ¨¡å‹å±¤ç´šçš„å¸è¼‰
- `enable_sequential_cpu_offload()`: **æ›´æ¿€é€²**,åºåˆ—åŒ–å¸è¼‰,åªä¿ç•™ç•¶å‰æ­¥é©Ÿåœ¨ GPU

**é æœŸæ•ˆæœ**: é™ä½ 2-3GB VRAM ä½¿ç”¨

### 2. GPU å¿«å–æ¸…ç†
```python
torch.cuda.empty_cache()  # æ¯æ¬¡ç”Ÿæˆå‰æ¸…ç†
```

### 3. CPU Generator
```python
generator=torch.Generator("cpu")  # é¿å… GPU åˆ†é…
```

## ğŸš€ é€²éšå„ªåŒ–æ–¹æ¡ˆ

å¦‚æœä¸Šè¿°å„ªåŒ–å¾Œä»æœ‰æº¢å‡º,è«‹ä¾åºå˜—è©¦:

### Level 2: é™ä½åœ–ç‰‡è§£æåº¦

ç·¨è¼¯ `app.py` ç¬¬ 105-106 è¡Œ:

```python
# å¾ 1024x1024 é™åˆ° 768x768 (æ¸›å°‘ ~30% VRAM)
height=768,
width=768,
```

æˆ–è€…æ›´æ¿€é€²:
```python
# é™åˆ° 512x512 (æ¸›å°‘ ~60% VRAM)
height=512,
width=512,
```

**VRAM ä½¿ç”¨æ¯”è¼ƒ**:
- 1024x1024: ~12-14 GB
- 768x768: ~8-10 GB âœ…
- 512x512: ~5-7 GB âœ…âœ…

### Level 3: ä½¿ç”¨ float16 ä»£æ›¿ bfloat16

ç·¨è¼¯ `app.py` ç¬¬ 37 è¡Œ:

```python
# åŸæœ¬
torch_dtype=torch.bfloat16,

# æ”¹ç‚º (å¯æ¸›å°‘ç´„ 10-15% VRAM)
torch_dtype=torch.float16,
```

**æ³¨æ„**: float16 åœ¨æŸäº›é¡¯å¡ä¸Šå¯èƒ½æ•¸å€¼ä¸ç©©å®š,å¦‚æœå‡ºç¾é»‘åœ–æˆ–ç•°å¸¸è«‹æ”¹å› bfloat16ã€‚

### Level 4: å•Ÿç”¨æ³¨æ„åŠ›åˆ‡ç‰‡ (Attention Slicing)

åœ¨ `initialize_model()` ä¸­æ·»åŠ :

```python
# åœ¨ enable_sequential_cpu_offload() ä¹‹å¾Œæ·»åŠ 
if hasattr(pipe, 'enable_attention_slicing'):
    pipe.enable_attention_slicing(1)  # æˆ– "auto"
    print("âœ“ å·²å•Ÿç”¨ Attention Slicing")
```

é€™æœƒå°‡æ³¨æ„åŠ›è¨ˆç®—åˆ†ç‰‡,æ¸›å°‘å³°å€¼ VRAM ä½¿ç”¨ã€‚

### Level 5: VAE Slicing (å¦‚æœæ”¯æ´)

```python
# åœ¨ initialize_model() ä¸­æ·»åŠ 
if hasattr(pipe, 'enable_vae_slicing'):
    pipe.enable_vae_slicing()
    print("âœ“ å·²å•Ÿç”¨ VAE Slicing")
```

## ğŸ“ å®Œæ•´å„ªåŒ–é…ç½®

### æ–¹æ¡ˆ A: ä¿æŒ 1024 è§£æåº¦ (æ¿€é€²å„ªåŒ–)

åœ¨ `initialize_model()` å‡½æ•¸ä¸­çš„ `enable_sequential_cpu_offload()` ä¹‹å¾Œæ·»åŠ :

```python
# å•Ÿç”¨æ‰€æœ‰å¯ç”¨çš„å„ªåŒ–
optimizations = []

# 1. Attention Slicing
if hasattr(pipe, 'enable_attention_slicing'):
    pipe.enable_attention_slicing("auto")
    optimizations.append("Attention Slicing")

# 2. VAE Slicing
if hasattr(pipe, 'enable_vae_slicing'):
    pipe.enable_vae_slicing()
    optimizations.append("VAE Slicing")

# 3. Memory Efficient Attention
if hasattr(pipe, 'enable_xformers_memory_efficient_attention'):
    try:
        pipe.enable_xformers_memory_efficient_attention()
        optimizations.append("xFormers Attention")
    except:
        pass

if optimizations:
    print(f"âœ“ å·²å•Ÿç”¨é¡å¤–å„ªåŒ–: {', '.join(optimizations)}")
```

### æ–¹æ¡ˆ B: é™ä½è§£æåº¦ (ç©©å®šæ–¹æ¡ˆ) âœ… æ¨è–¦

ä¿®æ”¹ç”Ÿæˆåƒæ•¸:

```python
image = pipe(
    prompt=prompt,
    height=768,   # å¾ 1024 é™åˆ° 768
    width=768,    # å¾ 1024 é™åˆ° 768
    num_inference_steps=9,
    guidance_scale=0.0,
    generator=torch.Generator("cpu").manual_seed(seed),
).images[0]
```

**å„ªé»**:
- âœ… VRAM ä½¿ç”¨é™è‡³ 8-10 GB
- âœ… å®Œå…¨é¿å…æº¢å‡º
- âœ… ç”Ÿæˆé€Ÿåº¦æ›´å¿« (ç´„ 5-8 ç§’)
- âœ… å“è³ªä»ç„¶å¾ˆå¥½

## ğŸ”§ å¯¦ç”¨å·¥å…·è…³æœ¬

å‰µå»ºä¸€å€‹æ¸¬è©¦è…³æœ¬ä¾†æ‰¾å‡ºæœ€ä½³é…ç½®:

```python
# test_vram_usage.py
import torch
import gc

def check_vram():
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0) / (1024**3)
        reserved = torch.cuda.memory_reserved(0) / (1024**3)
        total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"VRAM: å·²åˆ†é… {allocated:.2f}GB | å·²ä¿ç•™ {reserved:.2f}GB | ç¸½è¨ˆ {total:.2f}GB")
        return reserved
    return 0

# æ¸¬è©¦ä¸åŒè§£æåº¦çš„ VRAM ä½¿ç”¨
resolutions = [512, 768, 1024]
for res in resolutions:
    torch.cuda.empty_cache()
    gc.collect()
    print(f"\næ¸¬è©¦è§£æåº¦: {res}x{res}")
    # åœ¨é€™è£¡åŸ·è¡Œç”Ÿæˆä¸¦æª¢æŸ¥ VRAM
    check_vram()
```

## ğŸ¯ æ¨è–¦é…ç½® (12GB VRAM)

æ ¹æ“šæ‚¨çš„ RTX 4080 Laptop (12GB),æ¨è–¦é…ç½®:

### é¸é … 1: 768x768 (å¹³è¡¡) âœ…âœ…âœ…
```python
height=768, width=768
+ Sequential CPU Offload
+ GPU Cache Cleaning
```
- VRAM: ~8-10 GB (å®Œå…¨åœ¨å°ˆå±¬è¨˜æ†¶é«”å…§)
- é€Ÿåº¦: 5-8 ç§’/å¼µ
- å“è³ª: å„ªç§€

### é¸é … 2: 1024x1024 (é«˜å“è³ª)
```python
height=1024, width=1024
+ Sequential CPU Offload
+ Attention Slicing
+ VAE Slicing (å¦‚æœæ”¯æ´)
```
- VRAM: ~10-12 GB (å¯èƒ½è¼•å¾®æº¢å‡º)
- é€Ÿåº¦: 10-15 ç§’/å¼µ
- å“è³ª: æœ€ä½³

### é¸é … 3: 512x512 (æ¥µé€Ÿ)
```python
height=512, width=512
+ Sequential CPU Offload
```
- VRAM: ~5-7 GB
- é€Ÿåº¦: 3-5 ç§’/å¼µ
- å“è³ª: è‰¯å¥½ (é©åˆè‰åœ–æˆ–å¿«é€Ÿé è¦½)

## ğŸ“Š å„ªåŒ–æ•ˆæœå°æ¯”

| é…ç½® | VRAM ä½¿ç”¨ | æº¢å‡º | é€Ÿåº¦ | æ¨è–¦åº¦ |
|------|-----------|------|------|--------|
| **åŸå§‹ (pipe.to cuda)** | 21GB | âŒâŒâŒ åš´é‡ | 30-60s | âŒ |
| **Model CPU Offload** | 13.5GB | âŒ è¼•å¾® (1.8GB) | 15-20s | âš ï¸ |
| **Sequential CPU Offload** | 11-12GB | âš ï¸ é‚Šç·£ | 10-15s | âš ï¸ |
| **Sequential + 768** | 8-10GB | âœ… ç„¡ | 5-8s | âœ…âœ…âœ… |
| **Sequential + 512** | 5-7GB | âœ… ç„¡ | 3-5s | âœ…âœ… |

## ğŸ’¡ çµ‚æ¥µå»ºè­°

### ç«‹å³å¯è¡Œçš„æœ€ä½³æ–¹æ¡ˆ:

1. **ä¿®æ”¹ app.py å…©è™•**:

```python
# ç¬¬ 48 è¡Œ - æ”¹ç”¨ Sequential
pipe.enable_sequential_cpu_offload()

# ç¬¬ 105-106 è¡Œ - é™ä½è§£æåº¦
height=768,
width=768,
```

2. **é‡æ–°å•Ÿå‹•ä¼ºæœå™¨**:
```bash
python app.py
```

3. **ç”Ÿæˆä¸€å¼µåœ–ç‰‡ä¸¦æª¢æŸ¥**:
   - æ‰“é–‹å·¥ä½œç®¡ç†å“¡ â†’ æ•ˆèƒ½ â†’ GPU
   - æŸ¥çœ‹ã€Œå…±ç”¨ GPU è¨˜æ†¶é«”ã€æ˜¯å¦é‚„æœ‰æº¢å‡º
   - å¦‚æœä»æœ‰æº¢å‡º,å†é™åˆ° 512x512

### å¦‚æœéœ€è¦ 1024 è§£æåº¦

åªåœ¨å¿…è¦æ™‚ç”Ÿæˆ 1024,å¹³æ™‚ç”¨ 768:
- å¯ä»¥åœ¨ç¶²é æ·»åŠ è§£æåº¦é¸æ“‡å™¨
- æˆ–è€…æº–å‚™å…©å€‹å•Ÿå‹•é…ç½®

## ğŸ” ç›£æ§å‘½ä»¤

```bash
# æŒçºŒç›£æ§ GPU ä½¿ç”¨
nvidia-smi -l 1

# æˆ–ä½¿ç”¨ Python
watch -n 1 python check_model_cache.py
```

## âœ… ç¸½çµ

**æ¶ˆé™¤æº¢å‡ºçš„æœ€æœ‰æ•ˆæ–¹æ³•**: é™ä½è§£æåº¦åˆ° 768x768

é€™æ¨£å¯ä»¥:
- âœ… å®Œå…¨æ¶ˆé™¤å…±ç”¨è¨˜æ†¶é«”æº¢å‡º
- âœ… æå‡ç”Ÿæˆé€Ÿåº¦ (5-8 ç§’)
- âœ… ä¿æŒå„ªç§€çš„åœ–ç‰‡å“è³ª
- âœ… GPU ä½¿ç”¨ç‡æ›´ç©©å®š

è©¦è©¦çœ‹å§! ğŸš€
