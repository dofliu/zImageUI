import torch
import os
from diffusers import ZImagePipeline
from flask import Flask, render_template, request, jsonify, send_from_directory, send_file
from datetime import datetime
import base64
from io import BytesIO
import random
import config  # å°å…¥é…ç½®æª”æ¡ˆ
import json
import zipfile
import tempfile
from PIL import Image, ImageDraw, ImageFont

app = Flask(__name__)

# å¾é…ç½®æª”æ¡ˆè®€å–è·¯å¾‘è¨­å®š
cache_path = config.CACHE_PATH
output_path = config.OUTPUT_PATH
history_file = os.path.join(output_path, "history.json")
templates_file = os.path.join(os.path.dirname(__file__), "templates.json")
os.makedirs(cache_path, exist_ok=True)
os.makedirs(output_path, exist_ok=True)

# å…¨åŸŸè®Šæ•¸å­˜æ”¾æ¨¡å‹
pipe = None

# æ­·å²è¨˜éŒ„ç®¡ç†
def load_history():
    """è¼‰å…¥æ­·å²è¨˜éŒ„"""
    if os.path.exists(history_file):
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return []
    return []

def save_history(history):
    """å„²å­˜æ­·å²è¨˜éŒ„"""
    try:
        with open(history_file, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"å„²å­˜æ­·å²è¨˜éŒ„å¤±æ•—: {e}")

def add_to_history(prompt, filename, tags=None):
    """æ–°å¢æ­·å²è¨˜éŒ„"""
    history = load_history()
    history_item = {
        'id': f"{int(datetime.now().timestamp() * 1000)}_{random.randint(1000, 9999)}",
        'prompt': prompt,
        'filename': filename,
        'timestamp': datetime.now().isoformat(),
        'image_url': f'/images/{filename}',
        'tags': tags if tags else []
    }
    history.insert(0, history_item)  # æœ€æ–°çš„åœ¨å‰é¢
    # é™åˆ¶æ­·å²è¨˜éŒ„æ•¸é‡ (æœ€å¤š50ç­†)
    if len(history) > 50:
        history = history[:50]
    save_history(history)
    return history_item

def initialize_model():
    """åˆå§‹åŒ–æ¨¡å‹ (åªåŸ·è¡Œä¸€æ¬¡)"""
    global pipe
    if pipe is None:
        # æª¢æŸ¥æœ¬åœ°å¿«å–æ˜¯å¦å­˜åœ¨
        model_cache_exists = os.path.exists(os.path.join(cache_path, "models--Tongyi-MAI--Z-Image-Turbo"))
        if model_cache_exists:
            print("âœ“ ç™¼ç¾æœ¬åœ°å¿«å–,å¾ç¡¬ç¢Ÿè¼‰å…¥æ¨¡å‹...")
        else:
            print("âœ— æœªç™¼ç¾æœ¬åœ°å¿«å–,å°‡å¾ Hugging Face ä¸‹è¼‰æ¨¡å‹ (é€™éœ€è¦è¼ƒé•·æ™‚é–“)...")

        import time
        start_time = time.time()

        pipe = ZImagePipeline.from_pretrained(
            "Tongyi-MAI/Z-Image-Turbo",
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
            cache_dir=cache_path,
            use_safetensors=True,
            local_files_only=True,  # å…è¨±å¾ç¶²è·¯ä¸‹è¼‰,ä½†æœƒå„ªå…ˆä½¿ç”¨æœ¬åœ°å¿«å–
        )

        # é‡å° 12GB VRAM çš„å„ªåŒ–è¨­å®š
        print("âŸ³ å•Ÿç”¨ Sequential CPU Offload (æ›´æ¿€é€²çš„é¡¯å­˜å„ªåŒ–)...")
        pipe.enable_sequential_cpu_offload()
        # å•Ÿç”¨é¡å¤–çš„ VRAM å„ªåŒ–
        optimizations = []

        if config.ENABLE_ATTENTION_SLICING:
            if hasattr(pipe, 'enable_attention_slicing'):
                try:
                    pipe.enable_attention_slicing("auto")
                    optimizations.append("Attention Slicing")
                except Exception as e:
                    print(f"! Attention Slicing å•Ÿç”¨å¤±æ•—: {e}")

        if config.ENABLE_VAE_SLICING:
            if hasattr(pipe, 'enable_vae_slicing'):
                try:
                    pipe.enable_vae_slicing()
                    optimizations.append("VAE Slicing")
                except Exception as e:
                    print(f"! VAE Slicing å•Ÿç”¨å¤±æ•—: {e}")

        if config.ENABLE_XFORMERS:
            if hasattr(pipe, 'enable_xformers_memory_efficient_attention'):
                try:
                    pipe.enable_xformers_memory_efficient_attention()
                    optimizations.append("xFormers Attention")
                except Exception as e:
                    print(f"! xFormers å•Ÿç”¨å¤±æ•—: {e}")

        if optimizations:
            print(f"âœ“ å·²å•Ÿç”¨é¡å¤–å„ªåŒ–: {', '.join(optimizations)}")

        # å˜—è©¦å•Ÿç”¨ VAE Tiling (å¦‚æœ pipeline æ”¯æ´)
        try:
            if hasattr(pipe, 'enable_vae_tiling'):
                pipe.enable_vae_tiling()
                print("âœ“ å·²å•Ÿç”¨ VAE Tiling (å„ªåŒ–é«˜è§£æåº¦ç”Ÿæˆ)")
            else:
                print("! ZImagePipeline ä¸æ”¯æ´ VAE Tilingï¼Œè·³éæ­¤å„ªåŒ–")
        except Exception as e:
            print(f"! VAE Tiling å•Ÿç”¨å¤±æ•—: {e}")

        # å˜—è©¦å•Ÿç”¨ Flash Attention åŠ é€Ÿ (å¦‚æœç’°å¢ƒæ”¯æ´)
        try:
            if hasattr(pipe, 'transformer') and hasattr(pipe.transformer, 'set_attention_backend'):
                pipe.transformer.set_attention_backend("flash")
                print("âœ“ å·²å•Ÿç”¨ Flash Attention åŠ é€Ÿ")
            else:
                print("! Flash Attention ä¸å¯ç”¨ï¼Œä½¿ç”¨é è¨­ Attention")
        except Exception as e:
            print(f"! Flash Attention å•Ÿç”¨å¤±æ•—: {e}")

        # æ³¨æ„: å·²ä½¿ç”¨ enable_model_cpu_offload()ï¼Œä¸å†éœ€è¦ pipe.to("cuda")

        elapsed_time = time.time() - start_time
        print(f"âœ“ æ¨¡å‹è¼‰å…¥å®Œæˆ! (è€—æ™‚ {elapsed_time:.1f} ç§’)")
    else:
        print("âœ“ æ¨¡å‹å·²åœ¨è¨˜æ†¶é«”ä¸­,è·³éè¼‰å…¥")

@app.route('/')
def index():
    """é¦–é """
    return render_template('index.html')

@app.route('/generate', methods=['POST'])
def generate_image():
    """ç”Ÿæˆåœ–ç‰‡ API"""
    try:
        data = request.get_json()
        prompt = data.get('prompt', '')
        style_keywords = data.get('style_keywords', '')  # é¢¨æ ¼é—œéµå­—
        custom_width = data.get('width')  # è‡ªå®šç¾©å¯¬åº¦
        custom_height = data.get('height')  # è‡ªå®šç¾©é«˜åº¦

        if not prompt:
            return jsonify({'error': 'è«‹è¼¸å…¥æç¤ºè©'}), 400

        # çµ„åˆé¢¨æ ¼é—œéµå­—åˆ°æç¤ºè©
        if style_keywords:
            full_prompt = f"{prompt}, {style_keywords}"
        else:
            full_prompt = prompt

        # ç¢ºå®šä½¿ç”¨çš„å°ºå¯¸
        width = custom_width if custom_width else config.IMAGE_WIDTH
        height = custom_height if custom_height else config.IMAGE_HEIGHT

        # ç¢ºä¿æ¨¡å‹å·²è¼‰å…¥
        initialize_model()

        # ç”Ÿæˆå‰æ¸…ç† GPU å¿«å–
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("âœ“ å·²æ¸…ç† GPU å¿«å–")

        # ç”Ÿæˆåœ–åƒ
        print(f"é–‹å§‹ç”Ÿæˆï¼š{full_prompt}")
        if style_keywords:
            print(f"é¢¨æ ¼: {style_keywords}")
        # ä½¿ç”¨éš¨æ©Ÿç¨®å­,è®“æ¯æ¬¡ç”Ÿæˆçš„åœ–ç‰‡éƒ½ä¸åŒ
        seed = random.randint(0, 2**32 - 1)
        print(f"ä½¿ç”¨ç¨®å­: {seed}")

        # ä½¿ç”¨é…ç½®æª”æ¡ˆä¸­çš„åƒæ•¸ç”Ÿæˆåœ–ç‰‡
        print(f"ç”Ÿæˆè§£æåº¦: {width}x{height}")

        # ç”Ÿæˆåœ–ç‰‡
        # ä½¿ç”¨ CUDA generator ä»¥ç¢ºä¿èˆ‡æ¨¡å‹åœ¨åŒä¸€è¨­å‚™
        device = "cuda" if torch.cuda.is_available() else "cpu"
        generator = torch.Generator(device=device).manual_seed(seed)

        image = pipe(
            prompt=full_prompt,
            height=height,
            width=width,
            num_inference_steps=config.NUM_INFERENCE_STEPS,
            guidance_scale=config.GUIDANCE_SCALE,
            generator=generator,
        ).images[0]

        # ç”Ÿæˆå¸¶æœ‰æ—¥æœŸæ™‚é–“çš„æª”æ¡ˆåç¨±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"generated_{timestamp}.png"
        save_path = os.path.join(output_path, filename)

        # å„²å­˜åœ–ç‰‡
        image.save(save_path)
        print(f"åœ–ç‰‡å·²å„²å­˜è‡³ï¼š{save_path}")

        # æ·»åŠ åˆ°æ­·å²è¨˜éŒ„
        add_to_history(prompt, filename)

        # å°‡åœ–ç‰‡è½‰æ›ç‚º base64 ä»¥ä¾¿åœ¨ç¶²é ä¸Šé¡¯ç¤º
        buffered = BytesIO()
        image.save(buffered, format="PNG")
        img_str = base64.b64encode(buffered.getvalue()).decode()

        return jsonify({
            'success': True,
            'image': f"data:image/png;base64,{img_str}",
            'filename': filename,
            'prompt': prompt,
            'message': f'åœ–ç‰‡å·²æˆåŠŸç”Ÿæˆä¸¦å„²å­˜ç‚º {filename}'
        })

    except Exception as e:
        print(f"éŒ¯èª¤ï¼š{str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/images/<filename>')
def get_image(filename):
    """æä¾›åœ–ç‰‡ä¸‹è¼‰"""
    return send_from_directory(output_path, filename)

@app.route('/history', methods=['GET'])
def get_history():
    """ç²å–æ­·å²è¨˜éŒ„"""
    try:
        history = load_history()
        return jsonify({
            'success': True,
            'history': history
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/history', methods=['DELETE'])
def clear_history():
    """æ¸…é™¤æ‰€æœ‰æ­·å²è¨˜éŒ„"""
    try:
        save_history([])
        return jsonify({
            'success': True,
            'message': 'æ­·å²è¨˜éŒ„å·²æ¸…é™¤'
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/batch-generate', methods=['POST'])
def batch_generate():
    """æ‰¹é‡ç”Ÿæˆåœ–ç‰‡ API"""
    try:
        data = request.get_json()
        prompts = data.get('prompts', [])

        if not prompts or len(prompts) == 0:
            return jsonify({'error': 'è«‹è¼¸å…¥è‡³å°‘ä¸€å€‹æç¤ºè©'}), 400

        # é™åˆ¶æ‰¹é‡æ•¸é‡ (é¿å… VRAM å•é¡Œ)
        max_batch = 20
        if len(prompts) > max_batch:
            return jsonify({'error': f'æ‰¹é‡ç”Ÿæˆæœ€å¤šæ”¯æ´ {max_batch} å¼µåœ–ç‰‡'}), 400

        # ç¢ºä¿æ¨¡å‹å·²è¼‰å…¥
        initialize_model()

        results = []
        failed_prompts = []

        print(f"\né–‹å§‹æ‰¹é‡ç”Ÿæˆ {len(prompts)} å¼µåœ–ç‰‡...")

        for idx, prompt in enumerate(prompts, 1):
            prompt = prompt.strip()
            if not prompt:
                continue

            try:
                # ç”Ÿæˆå‰æ¸…ç† GPU å¿«å–
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

                print(f"\n[{idx}/{len(prompts)}] ç”Ÿæˆï¼š{prompt}")

                # ä½¿ç”¨éš¨æ©Ÿç¨®å­
                seed = random.randint(0, 2**32 - 1)
                print(f"ä½¿ç”¨ç¨®å­: {seed}")

                # ç”Ÿæˆåœ–ç‰‡
                device = "cuda" if torch.cuda.is_available() else "cpu"
                generator = torch.Generator(device=device).manual_seed(seed)

                image = pipe(
                    prompt=prompt,
                    height=config.IMAGE_HEIGHT,
                    width=config.IMAGE_WIDTH,
                    num_inference_steps=config.NUM_INFERENCE_STEPS,
                    guidance_scale=config.GUIDANCE_SCALE,
                    generator=generator,
                ).images[0]

                # ç”Ÿæˆæª”æ¡ˆåç¨±
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"batch_{timestamp}_{idx:03d}.png"
                save_path = os.path.join(output_path, filename)

                # å„²å­˜åœ–ç‰‡
                image.save(save_path)
                print(f"âœ“ åœ–ç‰‡å·²å„²å­˜: {filename}")

                # æ·»åŠ åˆ°æ­·å²è¨˜éŒ„
                add_to_history(prompt, filename)

                # è½‰æ›ç‚º base64
                buffered = BytesIO()
                image.save(buffered, format="PNG")
                img_str = base64.b64encode(buffered.getvalue()).decode()

                results.append({
                    'success': True,
                    'prompt': prompt,
                    'filename': filename,
                    'image': f"data:image/png;base64,{img_str}",
                    'index': idx
                })

            except Exception as e:
                print(f"âœ— ç”Ÿæˆå¤±æ•— [{idx}/{len(prompts)}]: {str(e)}")
                failed_prompts.append({
                    'prompt': prompt,
                    'index': idx,
                    'error': str(e)
                })
                results.append({
                    'success': False,
                    'prompt': prompt,
                    'error': str(e),
                    'index': idx
                })

        print(f"\næ‰¹é‡ç”Ÿæˆå®Œæˆ! æˆåŠŸ: {len(results) - len(failed_prompts)}/{len(prompts)}")

        return jsonify({
            'success': True,
            'total': len(prompts),
            'succeeded': len(results) - len(failed_prompts),
            'failed': len(failed_prompts),
            'results': results,
            'message': f'æ‰¹é‡ç”Ÿæˆå®Œæˆï¼ŒæˆåŠŸ {len(results) - len(failed_prompts)} å¼µï¼Œå¤±æ•— {len(failed_prompts)} å¼µ'
        })

    except Exception as e:
        print(f"æ‰¹é‡ç”ŸæˆéŒ¯èª¤ï¼š{str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/batch-download', methods=['POST'])
def batch_download():
    """æ‰¹é‡ä¸‹è¼‰åœ–ç‰‡ç‚º ZIP"""
    try:
        data = request.get_json()
        filenames = data.get('filenames', [])

        if not filenames:
            return jsonify({'error': 'æ²’æœ‰è¦ä¸‹è¼‰çš„æª”æ¡ˆ'}), 400

        # å»ºç«‹è‡¨æ™‚ ZIP æª”æ¡ˆ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        zip_filename = f"batch_images_{timestamp}.zip"
        zip_path = os.path.join(tempfile.gettempdir(), zip_filename)

        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for filename in filenames:
                file_path = os.path.join(output_path, filename)
                if os.path.exists(file_path):
                    zipf.write(file_path, filename)

        # ç™¼é€æª”æ¡ˆå¾Œåˆªé™¤è‡¨æ™‚æª”æ¡ˆ
        return send_file(
            zip_path,
            mimetype='application/zip',
            as_attachment=True,
            download_name=zip_filename
        )

    except Exception as e:
        print(f"æ‰¹é‡ä¸‹è¼‰éŒ¯èª¤ï¼š{str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/templates', methods=['GET'])
def get_templates():
    """ç²å–é¢¨æ ¼æ¨¡æ¿åˆ—è¡¨"""
    try:
        if os.path.exists(templates_file):
            with open(templates_file, 'r', encoding='utf-8') as f:
                templates = json.load(f)
            return jsonify({
                'success': True,
                'templates': templates
            })
        else:
            return jsonify({
                'success': False,
                'error': 'æ¨¡æ¿æª”æ¡ˆä¸å­˜åœ¨'
            }), 404
    except Exception as e:
        print(f"è®€å–æ¨¡æ¿éŒ¯èª¤ï¼š{str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/size-presets', methods=['GET'])
def get_size_presets():
    """ç²å–å°ºå¯¸é è¨­åˆ—è¡¨"""
    presets = {
        "ç¤¾ç¾¤åª’é«”": [
            {"name": "Instagram æ­£æ–¹å½¢", "width": 1080, "height": 1080, "ratio": "1:1"},
            {"name": "Instagram ç›´å¼", "width": 1080, "height": 1350, "ratio": "4:5"},
            {"name": "Facebook å°é¢", "width": 1200, "height": 630, "ratio": "1.91:1"},
            {"name": "Twitter å¡ç‰‡", "width": 1200, "height": 675, "ratio": "16:9"},
            {"name": "YouTube ç¸®åœ–", "width": 1280, "height": 720, "ratio": "16:9"}
        ],
        "åˆ—å°å°ºå¯¸": [
            {"name": "A4 ç›´å¼", "width": 2480, "height": 3508, "ratio": "A4"},
            {"name": "A4 æ©«å¼", "width": 3508, "height": 2480, "ratio": "A4"},
            {"name": "A5 ç›´å¼", "width": 1748, "height": 2480, "ratio": "A5"},
            {"name": "æ˜ä¿¡ç‰‡", "width": 1600, "height": 1200, "ratio": "4:3"}
        ],
        "æ¨™æº–å°ºå¯¸": [
            {"name": "æ­£æ–¹å½¢ 512", "width": 512, "height": 512, "ratio": "1:1", "vram": "ä½"},
            {"name": "æ­£æ–¹å½¢ 768", "width": 768, "height": 768, "ratio": "1:1", "vram": "ä¸­"},
            {"name": "æ­£æ–¹å½¢ 1024", "width": 1024, "height": 1024, "ratio": "1:1", "vram": "é«˜"},
            {"name": "å¯¬å± 16:9", "width": 1024, "height": 576, "ratio": "16:9", "vram": "ä¸­"},
            {"name": "ç›´å¼ 9:16", "width": 576, "height": 1024, "ratio": "9:16", "vram": "ä¸­"}
        ]
    }

    return jsonify({
        'success': True,
        'presets': presets,
        'current': {
            'width': config.IMAGE_WIDTH,
            'height': config.IMAGE_HEIGHT
        }
    })

@app.route('/tags', methods=['GET'])
def get_all_tags():
    """ç²å–æ‰€æœ‰ä½¿ç”¨éçš„æ¨™ç±¤"""
    try:
        history = load_history()
        all_tags = set()
        tag_counts = {}

        for item in history:
            if 'tags' in item:
                for tag in item['tags']:
                    all_tags.add(tag)
                    tag_counts[tag] = tag_counts.get(tag, 0) + 1

        # æŒ‰ä½¿ç”¨é »ç‡æ’åº
        sorted_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)

        return jsonify({
            'success': True,
            'tags': [{'name': tag, 'count': count} for tag, count in sorted_tags]
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/history/<item_id>/tags', methods=['POST'])
def update_tags(item_id):
    """æ›´æ–°æ­·å²è¨˜éŒ„çš„æ¨™ç±¤"""
    try:
        data = request.get_json()
        tags = data.get('tags', [])

        history = load_history()
        updated = False

        for item in history:
            if item.get('id') == item_id:
                item['tags'] = tags
                updated = True
                break

        if updated:
            save_history(history)
            return jsonify({
                'success': True,
                'message': 'æ¨™ç±¤å·²æ›´æ–°'
            })
        else:
            return jsonify({
                'success': False,
                'error': 'æ‰¾ä¸åˆ°è©²è¨˜éŒ„'
            }), 404
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/history/filter', methods=['POST'])
def filter_history():
    """æ ¹æ“šæ¨™ç±¤éæ¿¾æ­·å²è¨˜éŒ„"""
    try:
        data = request.get_json()
        filter_tags = data.get('tags', [])

        if not filter_tags:
            # æ²’æœ‰éæ¿¾æ¢ä»¶ï¼Œè¿”å›å…¨éƒ¨
            history = load_history()
        else:
            history = load_history()
            # éæ¿¾åŒ…å«ä»»ä¸€æ¨™ç±¤çš„è¨˜éŒ„
            filtered = [
                item for item in history
                if 'tags' in item and any(tag in item['tags'] for tag in filter_tags)
            ]
            history = filtered

        return jsonify({
            'success': True,
            'history': history,
            'count': len(history)
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/seed-control', methods=['POST'])
def generate_with_seed():
    """ä½¿ç”¨æŒ‡å®šç¨®å­ç”Ÿæˆåœ–ç‰‡ï¼ˆç”¨æ–¼é‡ç¾çµæœï¼‰"""
    try:
        data = request.get_json()
        prompt = data.get('prompt', '')
        seed = data.get('seed')
        style_keywords = data.get('style_keywords', '')
        custom_width = data.get('width')
        custom_height = data.get('height')

        if not prompt:
            return jsonify({'error': 'è«‹è¼¸å…¥æç¤ºè©'}), 400

        if seed is None:
            seed = random.randint(0, 2**32 - 1)

        # çµ„åˆé¢¨æ ¼é—œéµå­—
        if style_keywords:
            full_prompt = f"{prompt}, {style_keywords}"
        else:
            full_prompt = prompt

        # ç¢ºå®šå°ºå¯¸
        width = custom_width if custom_width else config.IMAGE_WIDTH
        height = custom_height if custom_height else config.IMAGE_HEIGHT

        # ç¢ºä¿æ¨¡å‹å·²è¼‰å…¥
        initialize_model()

        # ç”Ÿæˆå‰æ¸…ç† GPU å¿«å–
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        print(f"é–‹å§‹ç”Ÿæˆï¼ˆå›ºå®šç¨®å­ï¼‰ï¼š{full_prompt}")
        print(f"ç¨®å­: {seed}")
        print(f"è§£æåº¦: {width}x{height}")

        # ç”Ÿæˆåœ–ç‰‡
        device = "cuda" if torch.cuda.is_available() else "cpu"
        generator = torch.Generator(device=device).manual_seed(seed)

        image = pipe(
            prompt=full_prompt,
            height=height,
            width=width,
            num_inference_steps=config.NUM_INFERENCE_STEPS,
            guidance_scale=config.GUIDANCE_SCALE,
            generator=generator,
        ).images[0]

        # å„²å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"seed_{seed}_{timestamp}.png"
        save_path = os.path.join(output_path, filename)
        image.save(save_path)

        # æ·»åŠ åˆ°æ­·å²
        add_to_history(prompt, filename)

        # è½‰base64
        buffered = BytesIO()
        image.save(buffered, format="PNG")
        img_str = base64.b64encode(buffered.getvalue()).decode()

        return jsonify({
            'success': True,
            'image': f"data:image/png;base64,{img_str}",
            'filename': filename,
            'prompt': prompt,
            'seed': seed,
            'message': f'åœ–ç‰‡å·²ç”Ÿæˆï¼ˆç¨®å­: {seed}ï¼‰'
        })
    except Exception as e:
        print(f"éŒ¯èª¤ï¼š{str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/add-text-overlay', methods=['POST'])
def add_text_overlay():
    """åœ¨åœ–ç‰‡ä¸Šæ·»åŠ æ–‡å­—ç–ŠåŠ å±¤"""
    try:
        data = request.get_json()
        filename = data.get('filename', '')
        text = data.get('text', '')
        position = data.get('position', 'bottom')  # top, middle, bottom, custom
        text_color = data.get('text_color', 'white')  # white, black
        bg_overlay = data.get('bg_overlay', True)  # èƒŒæ™¯é®ç½©
        font_size = data.get('font_size', 48)  # å­—é«”å¤§å°
        custom_x = data.get('custom_x')  # è‡ªå®šç¾© X åº§æ¨™
        custom_y = data.get('custom_y')  # è‡ªå®šç¾© Y åº§æ¨™

        if not filename:
            return jsonify({'error': 'è«‹æä¾›åœ–ç‰‡æª”å'}), 400

        if not text:
            return jsonify({'error': 'è«‹è¼¸å…¥æ–‡å­—å…§å®¹'}), 400

        # è¼‰å…¥åŸåœ–
        image_path = os.path.join(output_path, filename)
        if not os.path.exists(image_path):
            return jsonify({'error': 'åœ–ç‰‡æª”æ¡ˆä¸å­˜åœ¨'}), 404

        image = Image.open(image_path)
        draw = ImageDraw.Draw(image, 'RGBA')

        # å˜—è©¦è¼‰å…¥ä¸­æ–‡å­—é«”ï¼ˆWindows ç³»çµ±ï¼‰
        try:
            # å¸¸è¦‹çš„ Windows ä¸­æ–‡å­—é«”è·¯å¾‘
            font_paths = [
                "C:/Windows/Fonts/msyh.ttc",  # å¾®è»Ÿé›…é»‘
                "C:/Windows/Fonts/msjh.ttc",  # å¾®è»Ÿæ­£é»‘é«”
                "C:/Windows/Fonts/simsun.ttc",  # å®‹é«”
                "C:/Windows/Fonts/simhei.ttf",  # é»‘é«”
            ]

            font = None
            for font_path in font_paths:
                if os.path.exists(font_path):
                    font = ImageFont.truetype(font_path, font_size)
                    print(f"âœ“ è¼‰å…¥å­—é«”: {font_path}")
                    break

            if font is None:
                # ä½¿ç”¨é è¨­å­—é«”
                font = ImageFont.load_default()
                print("âš  ä½¿ç”¨é è¨­å­—é«”ï¼ˆä¸æ”¯æ´ä¸­æ–‡ï¼‰")
        except Exception as e:
            print(f"å­—é«”è¼‰å…¥éŒ¯èª¤: {e}")
            font = ImageFont.load_default()

        # è¨ˆç®—æ–‡å­—å°ºå¯¸
        bbox = draw.textbbox((0, 0), text, font=font)
        text_width = bbox[2] - bbox[0]
        text_height = bbox[3] - bbox[1]

        img_width, img_height = image.size

        # è¨ˆç®—æ–‡å­—ä½ç½®
        if position == 'custom' and custom_x is not None and custom_y is not None:
            x = custom_x
            y = custom_y
        else:
            # æ°´å¹³ç½®ä¸­
            x = (img_width - text_width) // 2

            # å‚ç›´ä½ç½®
            if position == 'top':
                y = 50
            elif position == 'middle':
                y = (img_height - text_height) // 2
            else:  # bottom
                y = img_height - text_height - 50

        # ç¹ªè£½åŠé€æ˜èƒŒæ™¯é®ç½©
        if bg_overlay:
            padding = 20
            overlay_color = (0, 0, 0, 180) if text_color == 'white' else (255, 255, 255, 180)
            overlay_bbox = [
                x - padding,
                y - padding,
                x + text_width + padding,
                y + text_height + padding
            ]
            draw.rectangle(overlay_bbox, fill=overlay_color)

        # ç¹ªè£½æ–‡å­—
        text_rgb = (255, 255, 255) if text_color == 'white' else (0, 0, 0)
        draw.text((x, y), text, font=font, fill=text_rgb)

        # å„²å­˜æ–°åœ–ç‰‡
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        new_filename = f"text_overlay_{timestamp}.png"
        new_save_path = os.path.join(output_path, new_filename)
        image.save(new_save_path)
        print(f"âœ“ æ–‡å­—ç–ŠåŠ åœ–ç‰‡å·²å„²å­˜: {new_filename}")

        # æ·»åŠ åˆ°æ­·å²è¨˜éŒ„
        add_to_history(f"æ–‡å­—ç–ŠåŠ : {text}", new_filename)

        # è½‰æ›ç‚º base64
        buffered = BytesIO()
        image.save(buffered, format="PNG")
        img_str = base64.b64encode(buffered.getvalue()).decode()

        return jsonify({
            'success': True,
            'image': f"data:image/png;base64,{img_str}",
            'filename': new_filename,
            'original_filename': filename,
            'text': text,
            'message': 'æ–‡å­—ç–ŠåŠ å®Œæˆ'
        })

    except Exception as e:
        print(f"æ–‡å­—ç–ŠåŠ éŒ¯èª¤ï¼š{str(e)}")
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    # é¡¯ç¤ºé…ç½®è³‡è¨Š
    config.print_config_info()

    print(f"æ¨¡å‹ç·©å­˜è·¯å¾‘ï¼š{cache_path}")
    print(f"ç”Ÿæˆåœ–ç‰‡å„²å­˜è·¯å¾‘ï¼š{output_path}")

    # ğŸš€ å„ªåŒ–ï¼šåœ¨ä¼ºæœå™¨å•Ÿå‹•æ™‚å°±é è¼‰å…¥æ¨¡å‹
    print("\n===================================")
    print("æ­£åœ¨é è¼‰å…¥æ¨¡å‹...")
    print("===================================")
    initialize_model()
    print("âœ… æ¨¡å‹å·²å°±ç·’ï¼å¯ä»¥é–‹å§‹ç”Ÿæˆåœ–ç‰‡äº†\n")

    print("æ­£åœ¨å•Ÿå‹• Flask ä¼ºæœå™¨...")
    print(f"è«‹åœ¨ç€è¦½å™¨é–‹å•Ÿ: http://localhost:{config.PORT}")
    print("===================================\n")

    # é—œé–‰ reloader é¿å…ç”Ÿæˆéç¨‹ä¸­é‡æ–°è¼‰å…¥
    app.run(
        debug=config.DEBUG,
        host=config.HOST,
        port=config.PORT,
        use_reloader=config.USE_RELOADER
    )
