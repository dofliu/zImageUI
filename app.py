import torch
import os
from diffusers import ZImagePipeline
from flask import Flask, render_template, request, jsonify, send_from_directory
from datetime import datetime
import base64
from io import BytesIO
import random
import config  # å°å…¥é…ç½®æª”æ¡ˆ
import json

app = Flask(__name__)

# å¾é…ç½®æª”æ¡ˆè®€å–è·¯å¾‘è¨­å®š
cache_path = config.CACHE_PATH
output_path = config.OUTPUT_PATH
history_file = os.path.join(output_path, "history.json")
os.makedirs(cache_path, exist_ok=True)
os.makedirs(output_path, exist_ok=True)

# å…¨åŸŸè®Šæ•¸å­˜æ”¾æ¨¡å‹
pipe = None

# æ­·å²è¨˜éŒ„ç®¡ç†
def load_history():
    """è¼‰å…¥æ­·å²è¨˜éŒ„"""
    if os.path.exists(history_file):
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return []
    return []

def save_history(history):
    """å„²å­˜æ­·å²è¨˜éŒ„"""
    try:
        with open(history_file, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"å„²å­˜æ­·å²è¨˜éŒ„å¤±æ•—: {e}")

def add_to_history(prompt, filename):
    """æ–°å¢æ­·å²è¨˜éŒ„"""
    history = load_history()
    history_item = {
        'prompt': prompt,
        'filename': filename,
        'timestamp': datetime.now().isoformat(),
        'image_url': f'/images/{filename}'
    }
    history.insert(0, history_item)  # æœ€æ–°çš„åœ¨å‰é¢
    # é™åˆ¶æ­·å²è¨˜éŒ„æ•¸é‡ (æœ€å¤š50ç­†)
    if len(history) > 50:
        history = history[:50]
    save_history(history)
    return history_item

def initialize_model():
    """åˆå§‹åŒ–æ¨¡å‹ (åªåŸ·è¡Œä¸€æ¬¡)"""
    global pipe
    if pipe is None:
        # æª¢æŸ¥æœ¬åœ°å¿«å–æ˜¯å¦å­˜åœ¨
        model_cache_exists = os.path.exists(os.path.join(cache_path, "models--Tongyi-MAI--Z-Image-Turbo"))
        if model_cache_exists:
            print("âœ“ ç™¼ç¾æœ¬åœ°å¿«å–,å¾ç¡¬ç¢Ÿè¼‰å…¥æ¨¡å‹...")
        else:
            print("âœ— æœªç™¼ç¾æœ¬åœ°å¿«å–,å°‡å¾ Hugging Face ä¸‹è¼‰æ¨¡å‹ (é€™éœ€è¦è¼ƒé•·æ™‚é–“)...")

        import time
        start_time = time.time()

        pipe = ZImagePipeline.from_pretrained(
            "Tongyi-MAI/Z-Image-Turbo",
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
            cache_dir=cache_path,
            use_safetensors=True,
            local_files_only=False,  # å…è¨±å¾ç¶²è·¯ä¸‹è¼‰,ä½†æœƒå„ªå…ˆä½¿ç”¨æœ¬åœ°å¿«å–
        )

        # é‡å° 12GB VRAM çš„å„ªåŒ–è¨­å®š
        print("âŸ³ å•Ÿç”¨ Sequential CPU Offload (æ›´æ¿€é€²çš„é¡¯å­˜å„ªåŒ–)...")
        pipe.enable_sequential_cpu_offload()
        # å•Ÿç”¨é¡å¤–çš„ VRAM å„ªåŒ–
        optimizations = []

        if config.ENABLE_ATTENTION_SLICING:
            if hasattr(pipe, 'enable_attention_slicing'):
                try:
                    pipe.enable_attention_slicing("auto")
                    optimizations.append("Attention Slicing")
                except Exception as e:
                    print(f"! Attention Slicing å•Ÿç”¨å¤±æ•—: {e}")

        if config.ENABLE_VAE_SLICING:
            if hasattr(pipe, 'enable_vae_slicing'):
                try:
                    pipe.enable_vae_slicing()
                    optimizations.append("VAE Slicing")
                except Exception as e:
                    print(f"! VAE Slicing å•Ÿç”¨å¤±æ•—: {e}")

        if config.ENABLE_XFORMERS:
            if hasattr(pipe, 'enable_xformers_memory_efficient_attention'):
                try:
                    pipe.enable_xformers_memory_efficient_attention()
                    optimizations.append("xFormers Attention")
                except Exception as e:
                    print(f"! xFormers å•Ÿç”¨å¤±æ•—: {e}")

        if optimizations:
            print(f"âœ“ å·²å•Ÿç”¨é¡å¤–å„ªåŒ–: {', '.join(optimizations)}")

        # å˜—è©¦å•Ÿç”¨ VAE Tiling (å¦‚æœ pipeline æ”¯æ´)
        try:
            if hasattr(pipe, 'enable_vae_tiling'):
                pipe.enable_vae_tiling()
                print("âœ“ å·²å•Ÿç”¨ VAE Tiling (å„ªåŒ–é«˜è§£æåº¦ç”Ÿæˆ)")
            else:
                print("! ZImagePipeline ä¸æ”¯æ´ VAE Tilingï¼Œè·³éæ­¤å„ªåŒ–")
        except Exception as e:
            print(f"! VAE Tiling å•Ÿç”¨å¤±æ•—: {e}")

        # å˜—è©¦å•Ÿç”¨ Flash Attention åŠ é€Ÿ (å¦‚æœç’°å¢ƒæ”¯æ´)
        try:
            if hasattr(pipe, 'transformer') and hasattr(pipe.transformer, 'set_attention_backend'):
                pipe.transformer.set_attention_backend("flash")
                print("âœ“ å·²å•Ÿç”¨ Flash Attention åŠ é€Ÿ")
            else:
                print("! Flash Attention ä¸å¯ç”¨ï¼Œä½¿ç”¨é è¨­ Attention")
        except Exception as e:
            print(f"! Flash Attention å•Ÿç”¨å¤±æ•—: {e}")

        # æ³¨æ„: å·²ä½¿ç”¨ enable_model_cpu_offload()ï¼Œä¸å†éœ€è¦ pipe.to("cuda")

        elapsed_time = time.time() - start_time
        print(f"âœ“ æ¨¡å‹è¼‰å…¥å®Œæˆ! (è€—æ™‚ {elapsed_time:.1f} ç§’)")
    else:
        print("âœ“ æ¨¡å‹å·²åœ¨è¨˜æ†¶é«”ä¸­,è·³éè¼‰å…¥")

@app.route('/')
def index():
    """é¦–é """
    return render_template('index.html')

@app.route('/generate', methods=['POST'])
def generate_image():
    """ç”Ÿæˆåœ–ç‰‡ API"""
    try:
        data = request.get_json()
        prompt = data.get('prompt', '')

        if not prompt:
            return jsonify({'error': 'è«‹è¼¸å…¥æç¤ºè©'}), 400

        # ç¢ºä¿æ¨¡å‹å·²è¼‰å…¥
        initialize_model()

        # ç”Ÿæˆå‰æ¸…ç† GPU å¿«å–
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("âœ“ å·²æ¸…ç† GPU å¿«å–")

        # ç”Ÿæˆåœ–åƒ
        print(f"é–‹å§‹ç”Ÿæˆï¼š{prompt}")
        # ä½¿ç”¨éš¨æ©Ÿç¨®å­,è®“æ¯æ¬¡ç”Ÿæˆçš„åœ–ç‰‡éƒ½ä¸åŒ
        seed = random.randint(0, 2**32 - 1)
        print(f"ä½¿ç”¨ç¨®å­: {seed}")

        # ä½¿ç”¨é…ç½®æª”æ¡ˆä¸­çš„åƒæ•¸ç”Ÿæˆåœ–ç‰‡
        print(f"ç”Ÿæˆè§£æåº¦: {config.IMAGE_WIDTH}x{config.IMAGE_HEIGHT}")

        # ç”Ÿæˆåœ–ç‰‡
        # ä½¿ç”¨ CUDA generator ä»¥ç¢ºä¿èˆ‡æ¨¡å‹åœ¨åŒä¸€è¨­å‚™
        device = "cuda" if torch.cuda.is_available() else "cpu"
        generator = torch.Generator(device=device).manual_seed(seed)

        image = pipe(
            prompt=prompt,
            height=config.IMAGE_HEIGHT,
            width=config.IMAGE_WIDTH,
            num_inference_steps=config.NUM_INFERENCE_STEPS,
            guidance_scale=config.GUIDANCE_SCALE,
            generator=generator,
        ).images[0]

        # ç”Ÿæˆå¸¶æœ‰æ—¥æœŸæ™‚é–“çš„æª”æ¡ˆåç¨±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"generated_{timestamp}.png"
        save_path = os.path.join(output_path, filename)

        # å„²å­˜åœ–ç‰‡
        image.save(save_path)
        print(f"åœ–ç‰‡å·²å„²å­˜è‡³ï¼š{save_path}")

        # æ·»åŠ åˆ°æ­·å²è¨˜éŒ„
        add_to_history(prompt, filename)

        # å°‡åœ–ç‰‡è½‰æ›ç‚º base64 ä»¥ä¾¿åœ¨ç¶²é ä¸Šé¡¯ç¤º
        buffered = BytesIO()
        image.save(buffered, format="PNG")
        img_str = base64.b64encode(buffered.getvalue()).decode()

        return jsonify({
            'success': True,
            'image': f"data:image/png;base64,{img_str}",
            'filename': filename,
            'prompt': prompt,
            'message': f'åœ–ç‰‡å·²æˆåŠŸç”Ÿæˆä¸¦å„²å­˜ç‚º {filename}'
        })

    except Exception as e:
        print(f"éŒ¯èª¤ï¼š{str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/images/<filename>')
def get_image(filename):
    """æä¾›åœ–ç‰‡ä¸‹è¼‰"""
    return send_from_directory(output_path, filename)

@app.route('/history', methods=['GET'])
def get_history():
    """ç²å–æ­·å²è¨˜éŒ„"""
    try:
        history = load_history()
        return jsonify({
            'success': True,
            'history': history
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/history', methods=['DELETE'])
def clear_history():
    """æ¸…é™¤æ‰€æœ‰æ­·å²è¨˜éŒ„"""
    try:
        save_history([])
        return jsonify({
            'success': True,
            'message': 'æ­·å²è¨˜éŒ„å·²æ¸…é™¤'
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    # é¡¯ç¤ºé…ç½®è³‡è¨Š
    config.print_config_info()

    print(f"æ¨¡å‹ç·©å­˜è·¯å¾‘ï¼š{cache_path}")
    print(f"ç”Ÿæˆåœ–ç‰‡å„²å­˜è·¯å¾‘ï¼š{output_path}")

    # ğŸš€ å„ªåŒ–ï¼šåœ¨ä¼ºæœå™¨å•Ÿå‹•æ™‚å°±é è¼‰å…¥æ¨¡å‹
    print("\n===================================")
    print("æ­£åœ¨é è¼‰å…¥æ¨¡å‹...")
    print("===================================")
    initialize_model()
    print("âœ… æ¨¡å‹å·²å°±ç·’ï¼å¯ä»¥é–‹å§‹ç”Ÿæˆåœ–ç‰‡äº†\n")

    print("æ­£åœ¨å•Ÿå‹• Flask ä¼ºæœå™¨...")
    print(f"è«‹åœ¨ç€è¦½å™¨é–‹å•Ÿ: http://localhost:{config.PORT}")
    print("===================================\n")

    # é—œé–‰ reloader é¿å…ç”Ÿæˆéç¨‹ä¸­é‡æ–°è¼‰å…¥
    app.run(
        debug=config.DEBUG,
        host=config.HOST,
        port=config.PORT,
        use_reloader=config.USE_RELOADER
    )
